import feedparser
from bs4 import BeautifulSoup
from datetime import datetime
from .scraper_fetcher import (
    extract_keywords,
    infer_article_type,
)  # Ù„Ø¥Ø¶Ø§ÙØ© Ù†ÙˆØ¹ Ø§Ù„Ù…Ù‚Ø§Ù„ ÙˆØ§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©


def fetch_from_rss(source):
    try:
        feed = feedparser.parse(source.news_source_url)
        articles = []

        for entry in feed.entries[:10]:
            title = entry.title
            content_html = entry.get("summary", "") or entry.get("content", [{}])[
                0
            ].get("value", "")
            published_at = None

            # ğŸ“… ØªØ§Ø±ÙŠØ® Ø§Ù„Ù†Ø´Ø±
            if hasattr(entry, "published_parsed"):
                published_at = datetime(*entry.published_parsed[:6])

            # ğŸ–¼ï¸ Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØµÙˆØ±Ø© Ø¥Ø°Ø§ Ù…ÙˆØ¬ÙˆØ¯Ø©
            soup = BeautifulSoup(content_html, "html.parser")
            img_tag = soup.find("img")
            image = img_tag.get("src") if img_tag else None

            # ğŸ”‘ ÙƒÙ„Ù…Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ©
            keywords = extract_keywords(title + " " + soup.get_text())

            # ğŸ§  Ù†ÙˆØ¹ Ø§Ù„Ù…Ù‚Ø§Ù„ (Ø³ÙŠØ§Ø³Ø©ØŒ ØµØ­Ø©ØŒ Ø±ÙŠØ§Ø¶Ø©..)
            article_type = infer_article_type(title)

            # ğŸ–‹ï¸ Ø§Ù„Ø±Ø§Ø¨Ø· (Ù…Ù‡Ù… Ø¬Ø¯Ù‹Ø§ Ù„Ùˆ Ø¨Ø¯Ùƒ ØªØ²ÙˆØ±Ù‡ Ù„Ø§Ø­Ù‚Ù‹Ø§)
            link = entry.get("link", None)

            articles.append(
                {
                    "title": title,
                    "content": soup.get_text(),
                    "keywords": keywords,
                    "image": image,
                    "published_at": published_at,
                    "type": article_type,
                    "category": None,
                    "url": link,
                }
            )

        return articles

    except Exception as e:
        print(f"[RSS ERROR] {source.news_source_name}: {e}")
        return []

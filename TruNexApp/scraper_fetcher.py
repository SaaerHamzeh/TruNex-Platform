import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from dateutil import parser
from .models import NewsSourceSectionURL
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from django.utils import timezone
import time


def get_section_url(source, target):
    try:
        section = source.section_urls.get(section_type=target)
        return section.section_url
    except:
        return None


def fetch_from_scraper(source, target="main"):

    # âš™ï¸ ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø±Ø§Ø¨Ø· Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù†ÙˆØ¹ target
    if isinstance(target, NewsSourceSectionURL):
        url = target.section_url
        section_type = target.section_type
        section_region = target.section_region
    else:
        from .scraper_fetcher import get_section_url

        url = get_section_url(source, target) or source.news_source_url
        section_type = target
        section_region = "unknown"

    print(
        f"ğŸ§­ Scraping {source.news_source_name} - {section_type or 'main'} - {section_region or 'unknown'} from: {url}"
    )

    try:
        response = requests.get(url, timeout=10)
        soup = BeautifulSoup(response.content, "html.parser")
        # if "sana.sy" in url:
        #     return scrape_sana_with_selenium(source, override_url=url)
        if "alhadath" in url:
            return scrape_alhadath_with_selenium(source, override_url=url)
        # elif "elsport" in url:
        #     return scrape_elsport(source, override_url=url)
        # elif "syria.tv" in url:
        #     return scrape_syria_tv_with_selenium(source, override_url=url)
        elif "aljazeera" in url:
            return scrape_aljazeera(source)
        # elif "bbc" in url:
        #     return scrape_bbc(soup, source)
        else:
            print(f"âš ï¸ No custom scraper defined for {url}")
            return []

    except Exception as e:
        print(f"[SCRAPER ERROR] {source.news_source_name}: {e}")
        return []


def infer_article_type(title: str) -> str:
    import re

    # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¹Ù†ÙˆØ§Ù†
    title = re.sub(r"[^\w\sØ¡-ÙŠ]", "", title.lower())

    CATEGORY_KEYWORDS = {
        "politic": [
            "Ø³ÙŠØ§Ø³Ø©",
            "Ø­ÙƒÙˆÙ…Ø©",
            "Ø±Ø¦ÙŠØ³",
            "ÙˆØ²ÙŠØ±",
            "Ø±Ø¦Ø§Ø³Ø©",
            "Ø§Ù†ØªØ®Ø§Ø¨Ø§Øª",
            "Ø¯Ø³ØªÙˆØ±",
            "Ø¨Ø±Ù„Ù…Ø§Ù†",
            "Ù…Ø¬Ù„Ø³",
            "Ø¯ÙˆÙ„Ø©",
            "Ø²Ø¹ÙŠÙ…",
            "Ø§Ø­ØªÙ„Ø§Ù„",
            "Ø¥Ø³Ø±Ø§Ø¦ÙŠÙ„",
            "ØªØ±Ø§Ù…Ø¨",
            "Ø¨Ø§ÙŠØ¯Ù†",
            "Ù…Ø¹Ø§Ù‡Ø¯Ø©",
            "ØµØ±Ø§Ø¹",
            "Ù†Ø²Ø§Ø¹",
            "Ø³Ù„Ø·Ø©",
            "Ù…Ø±Ø³ÙˆÙ…",
            "politics",
            "government",
            "president",
            "minister",
            "parliament",
            "election",
            "constitution",
            "conflict",
            "state",
            "leader",
            "treaty",
            "authority",
            "trump",
            "biden",
            "israel",
        ],
        "sports": [
            "Ø±ÙŠØ§Ø¶Ø©",
            "Ø¯ÙˆØ±ÙŠ",
            "ÙƒØ£Ø³",
            "Ù…Ø¨Ø§Ø±Ø§Ø©",
            "Ù„Ø§Ø¹Ø¨",
            "Ù…Ù†ØªØ®Ø¨",
            "Ø¨Ø·ÙˆÙ„Ø©",
            "Ø¬ÙˆÙ„",
            "Ù‡Ø¯Ù",
            "Ø±ÙƒÙ„Ø©",
            "Ù…Ø¯Ø±Ø¨",
            "Ø´ÙˆØ·",
            "Ù…ÙŠØ³ÙŠ",
            "Ø±ÙˆÙ†Ø§Ù„Ø¯Ùˆ",
            "Ø¨Ø±Ø´Ù„ÙˆÙ†Ø©",
            "Ø±ÙŠØ§Ù„",
            "Ø§Ù„Ø§ØªØ­Ø§Ø¯",
            "Ø§Ù„Ù‡Ù„Ø§Ù„",
            "sport",
            "match",
            "game",
            "team",
            "league",
            "championship",
            "goal",
            "player",
            "coach",
            "half",
            "cup",
            "messi",
            "ronaldo",
            "barcelona",
            "madrid",
            "alhilal",
            "ittihad",
        ],
        "health": [
            "ØµØ­Ø©",
            "Ù…Ø±ÙŠØ¶",
            "Ø¯ÙˆØ§Ø¡",
            "Ø¯ÙˆØ§Ø¦ÙŠØ©",
            "Ù…Ø³ØªØ´ÙÙ‰",
            "Ø·Ø¨ÙŠØ¨",
            "Ø¬Ø±Ø§Ø­Ø©",
            "Ù„Ù‚Ø§Ø­",
            "ÙƒÙˆØ±ÙˆÙ†Ø§",
            "ÙØ§ÙŠØ±ÙˆØ³",
            "Ù…Ø±Ø¶",
            "ÙˆØ¨Ø§Ø¡",
            "Ø¥ØµØ§Ø¨Ø©",
            "ØªØ­Ø§Ù„ÙŠÙ„",
            "Ø£Ø¹Ø±Ø§Ø¶",
            "Ù†Ù‚Ù„ Ø¯Ù…",
            "Ø¹ÙŠØ§Ø¯Ø©",
            "Ø¶ØºØ·",
            "Ø³ÙƒØ±ÙŠ",
            "ÙƒÙˆÙ„ÙŠØ³ØªØ±ÙˆÙ„",
            "Ø¥Ù†ÙÙ„ÙˆÙ†Ø²Ø§",
            "health",
            "hospital",
            "doctor",
            "surgery",
            "medicine",
            "vaccine",
            "covid",
            "pandemic",
            "virus",
            "infection",
            "disease",
            "symptoms",
            "clinic",
            "test",
            "flu",
            "blood pressure",
            "diabetes",
            "cholesterol",
        ],
        "economy": [
            "Ø§Ù‚ØªØµØ§Ø¯",
            "Ù…Ø§Ù„",
            "Ø³ÙˆÙ‚",
            "Ø¨ÙˆØ±ØµØ©",
            "ØªØ¶Ø®Ù…",
            "Ø¹Ù…Ù„Ø©",
            "Ø¯ÙˆÙ„Ø§Ø±",
            "ÙŠÙˆØ±Ùˆ",
            "Ø°Ù‡Ø¨",
            "Ù†ÙØ·",
            "Ø£Ø³Ø¹Ø§Ø±",
            "ØºÙ„Ø§Ø¡",
            "Ø¯Ø®Ù„",
            "Ø±ÙˆØ§ØªØ¨",
            "Ø¶Ø±Ø§Ø¦Ø¨",
            "Ø¨Ù†Ùƒ",
            "Ù‚Ø±Ø¶",
            "ØªÙ…ÙˆÙŠÙ„",
            "Ø§Ø³ØªØ«Ù…Ø§Ø±",
            "Ø¹Ø¬Ø²",
            "Ù…ÙŠØ²Ø§Ù†ÙŠØ©",
            "economy",
            "finance",
            "market",
            "stock",
            "dollar",
            "euro",
            "gold",
            "oil",
            "price",
            "income",
            "salary",
            "bank",
            "loan",
            "budget",
            "investment",
            "deficit",
            "tax",
            "inflation",
            "currency",
        ],
        "education": [
            "ØªØ¹Ù„ÙŠÙ…",
            "Ø¯Ø±Ø§Ø³Ø©",
            "Ù…Ø¯Ø±Ø³Ø©",
            "Ø·Ù„Ø§Ø¨",
            "Ø§Ù…ØªØ­Ø§Ù†",
            "Ø´Ù‡Ø§Ø¯Ø©",
            "Ù…Ù†Ù‡Ø§Ø¬",
            "Ø¹Ù„Ø§Ù…Ø§Øª",
            "Ù†ØªØ§Ø¦Ø¬",
            "Ù†Ø¬Ø§Ø­",
            "Ø³Ù†Ø© Ø¯Ø±Ø§Ø³ÙŠØ©",
            "Ù…Ø¯Ø±Ø³ÙŠ",
            "Ø¬Ø§Ù…Ø¹Ø©",
            "Ø¯ÙƒØªÙˆØ±Ø§Ù‡",
            "Ù…Ø§Ø¬Ø³ØªÙŠØ±",
            "Ù…Ù‚Ø±Ø±",
            "ØªØ­ØµÙŠÙ„",
            "Ø§Ø®ØªØ¨Ø§Ø±",
            "ØªØ¹Ù„ÙŠÙ… Ø¹Ù† Ø¨Ø¹Ø¯",
            "education",
            "study",
            "school",
            "student",
            "exam",
            "certificate",
            "grade",
            "result",
            "university",
            "phd",
            "masters",
            "curriculum",
            "distance learning",
            "academic",
            "syllabus",
            "semester",
        ],
        "technology": [
            "ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§",
            "ØªÙ‚Ù†ÙŠØ©",
            "Ù‡Ø§ØªÙ",
            "Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ",
            "Ø±ÙˆØ¨ÙˆØª",
            "Ø¨Ø±Ù…Ø¬ÙŠØ§Øª",
            "ØªØ·Ø¨ÙŠÙ‚",
            "Ø¨Ø±Ù†Ø§Ù…Ø¬",
            "Ø­Ø§Ø³ÙˆØ¨",
            "Ø¥Ù†ØªØ±Ù†Øª",
            "ØªØ´ÙÙŠØ±",
            "Ù…ÙˆØ¨Ø§ÙŠÙ„",
            "Ø¨Ø±Ù…Ø¬Ø©",
            "Ø´Ø¨ÙƒØ©",
            "Ø³ÙŠÙ„ÙŠÙƒÙˆÙ†",
            "ÙƒØ§Ù…ÙŠØ±Ø§",
            "Ø´Ø§Ø´Ø©",
            "Ù…Ø¹Ø§Ù„Ø¬",
            "Ø£Ù„Ø¹Ø§Ø¨",
            "ØªØ´Ø§Øª Ø¬ÙŠ Ø¨ÙŠ ØªÙŠ",
            "ÙƒÙˆØ¯",
            "ÙˆØ§Ù‚Ø¹ Ø§ÙØªØ±Ø§Ø¶ÙŠ",
            "Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ©",
            "technology",
            "tech",
            "ai",
            "artificial intelligence",
            "robot",
            "software",
            "app",
            "program",
            "mobile",
            "internet",
            "encryption",
            "vr",
            "virtual reality",
            "hardware",
            "processor",
            "code",
            "gaming",
            "machine learning",
        ],
        "space": [
            "ÙØ¶Ø§Ø¡",
            "Ù†Ø§Ø³Ø§",
            "Ù‚Ù…Ø±",
            "Ù…Ø±ÙŠØ®",
            "Ù…Ø±ÙƒØ¨Ø© ÙØ¶Ø§Ø¦ÙŠØ©",
            "Ù…Ø¬Ø±Ø©",
            "ÙƒÙˆÙƒØ¨",
            "ØªÙ„Ø³ÙƒÙˆØ¨",
            "Ø±Ø§Ø¦Ø¯ ÙØ¶Ø§Ø¡",
            "ØµØ§Ø±ÙˆØ®",
            "Ù…Ù‡Ù…Ø©",
            "space",
            "nasa",
            "moon",
            "mars",
            "galaxy",
            "planet",
            "telescope",
            "rocket",
            "astronaut",
            "mission",
        ],
        "security": [
            "Ø£Ù…Ù†",
            "Ù‡Ø¬ÙˆÙ…",
            "Ø§Ø®ØªØ±Ø§Ù‚",
            "Ù‚Ø±Ø§ØµÙ†Ø©",
            "ØªØ³Ø±ÙŠØ¨",
            "Ù‡Ø¬ÙˆÙ… Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ",
            "Ø­Ù…Ø§ÙŠØ©",
            "ØªØ´ÙÙŠØ±",
            "Ø¯ÙØ§Ø¹",
            "Ù…Ø®Ø§Ø¨Ø±Ø§Øª",
            "ØªÙ‡Ø¯ÙŠØ¯",
            "ØªÙØ¬ÙŠØ±",
            "Ø¥Ø±Ù‡Ø§Ø¨",
            "security",
            "cyber",
            "hacking",
            "hacker",
            "breach",
            "leak",
            "cyberattack",
            "defense",
            "encryption",
            "intelligence",
            "terrorism",
        ],
    }

    for category, keywords in CATEGORY_KEYWORDS.items():
        if any(word in title for word in keywords):
            return category

    return "general"


def extract_keywords(text, max_words=7):
    import re
    from collections import Counter

    # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Øµ ÙˆØªØ­ÙˆÙŠÙ„Ù‡ Ø¥Ù„Ù‰ Ø­Ø±ÙˆÙ ØµØºÙŠØ±Ø©
    text = re.sub(r"[^\w\sØ¡-ÙŠ]", "", text.lower())

    # ØªØ¬Ø²Ø¦Ø© Ø¥Ù„Ù‰ ÙƒÙ„Ù…Ø§Øª
    words = re.findall(r"\b[\wØ¡-ÙŠ]+\b", text)

    # Ù‚Ø§Ø¦Ù…Ø© ÙƒÙ„Ù…Ø§Øª Ø§Ù„ØªÙˆÙ‚Ù Ø§Ù„Ù…ÙˆØ³Ø¹Ø© (Ø¹Ø±Ø¨ÙŠ + Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠ)
    stopwords = set(
        [
            # Ø¹Ø±Ø¨ÙŠØ©
            "ÙÙŠ",
            "Ù…Ù†",
            "Ø¹Ù„Ù‰",
            "Ø¹Ù†",
            "Ø¥Ù„Ù‰",
            "Ùˆ",
            "Ù…Ø¹",
            "Ø£Ù†",
            "Ù‡Ø°Ø§",
            "Ù…Ø§",
            "Ù„Ù…",
            "ÙƒØ§Ù†",
            "Ù‚Ø¯",
            "Ù„Ø§",
            "Ù‡Ùˆ",
            "Ù‡ÙŠ",
            "ÙƒÙ…Ø§",
            "Ø°Ù„Ùƒ",
            "Ø¨Ø¹Ø¯",
            "Ø£Ùˆ",
            "ÙƒÙ„",
            "Ø£ÙŠ",
            "ØªÙ…",
            "ØªÙƒÙˆÙ†",
            "Ù‚Ø¨Ù„",
            "Ø¨ÙŠÙ†",
            "Ø£ÙƒØ«Ø±",
            "Ø£Ù‚Ù„",
            "Ù…Ù†Ø°",
            "Ø­ØªÙ‰",
            "Ø­ÙŠØ«",
            "Ø¥Ù„Ø§",
            "Ø£Ø­Ø¯",
            "Ø£Ø®Ø±Ù‰",
            "Ø£Ù…Ø§Ù…",
            "Ù„Ø¯Ù‰",
            "Ù„Ø°Ù„Ùƒ",
            # Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©
            "the",
            "and",
            "for",
            "with",
            "that",
            "this",
            "are",
            "was",
            "were",
            "has",
            "have",
            "had",
            "been",
            "from",
            "not",
            "will",
            "would",
            "can",
            "could",
            "should",
            "you",
            "your",
            "they",
            "their",
            "but",
            "about",
            "what",
            "when",
            "where",
            "which",
            "who",
            "whom",
            "how",
            "why",
            "there",
        ]
    )

    # ØªØµÙÙŠØ© Ø§Ù„ÙƒÙ„Ù…Ø§Øª
    filtered = [word for word in words if word not in stopwords and len(word) > 2]

    # Ø¥Ø­ØµØ§Ø¡ Ø§Ù„ØªÙƒØ±Ø§Ø± ÙˆØ§Ø®ØªÙŠØ§Ø± Ø§Ù„Ø£ÙƒØ«Ø± Ø´ÙŠÙˆØ¹Ù‹Ø§
    most_common = Counter(filtered).most_common(max_words)

    # Ø¥Ø±Ø¬Ø§Ø¹Ù‡Ø§ ÙƒØ³Ù„Ø³Ù„Ø© Ù…ÙØµÙˆÙ„Ø© Ø¨ÙÙˆØ§ØµÙ„
    return ", ".join(word for word, _ in most_common)


# --------------Ø§Ù„Ø­Ø¯Ø«--------------
def scrape_alhadath_with_selenium(source, override_url=None):

    # âœ… Ø¥Ø¬Ø¨Ø§Ø± Ø§Ù„Ø°Ù‡Ø§Ø¨ Ù„Ù„ØµÙØ­Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© ÙÙ‚Ø·
    url = source.news_source_url

    options = Options()
    options.headless = True
    options.add_argument("--disable-blink-features=AutomationControlled")
    service = Service("D:/chromedriver/chromedriver.exe")
    driver = webdriver.Chrome(service=service, options=options)

    articles = []

    try:
        driver.get(url)
        time.sleep(5)
        soup = BeautifulSoup(driver.page_source, "html.parser")

        cards = soup.select("li.list-item")
        print(f"ğŸ§© Found {len(cards)} cards in AlHadath")

        for card in cards[:10]:
            try:
                link_tag = card.select_one("a.article-content-wrapper")
                full_link = (
                    urljoin(source.news_source_url, link_tag["href"])
                    if link_tag and link_tag.has_attr("href")
                    else "#"
                )

                title_tag = card.select_one("span.title")
                title = title_tag.get_text(strip=True) if title_tag else "Ø¨Ø¯ÙˆÙ† Ø¹Ù†ÙˆØ§Ù†"

                image_tag = card.select_one("span.img-box img")
                image_url = (
                    urljoin(source.news_source_url, image_tag["src"])
                    if image_tag and image_tag.has_attr("src")
                    else None
                )

                # ğŸ“ Ù†ÙƒØªÙÙŠ Ø¨Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© ÙÙ‚Ø·
                article_content = title
                published_at = None

                articles.append(
                    {
                        "title": title,
                        "content": article_content,
                        "keywords": extract_keywords(title + " " + article_content),
                        "image": image_url,
                        "published_at": published_at,
                        "type": infer_article_type(title),
                        "category": None,
                        "url": full_link,
                    }
                )

            except Exception as e:
                print(f"âš ï¸ AlHadath card parsing error: {e}")

    except Exception as e:
        print(f"âŒ Selenium failed for AlHadath: {e}")

    finally:
        driver.quit()

    return articles


# --------------Ø³Ø§Ù†Ø§--------------
def scrape_sana_with_selenium(source, override_url=None):
    from selenium import webdriver
    from selenium.webdriver.chrome.options import Options
    from selenium.webdriver.chrome.service import Service
    from bs4 import BeautifulSoup
    from urllib.parse import urljoin
    import time

    url = override_url or source.news_source_url

    options = Options()
    options.headless = True
    service = Service("D:/chromedriver/chromedriver.exe")
    driver = webdriver.Chrome(service=service, options=options)

    articles = []
    try:
        driver.get(url)
        time.sleep(5)  # Ø¨Ù…Ù‡Ù„ Ù„ÙŠØ­Ù…Ù‘Ù„ Ø§Ù„ØµÙˆØ± ÙˆØ§Ù„Ù€ JS
        soup = BeautifulSoup(driver.page_source, "html.parser")

        cards = soup.select("article.item-list")

        print(f"ğŸ§© Found {len(cards)} articles in SANA [via Selenium]")

        for card in cards[:5]:
            try:
                title_tag = card.select_one("h2.post-box-title a")
                title = title_tag.get_text(strip=True) if title_tag else "Ø¨Ø¯ÙˆÙ† Ø¹Ù†ÙˆØ§Ù†"
                full_link = (
                    urljoin(source.news_source_url, title_tag["href"])
                    if title_tag
                    else "#"
                )

                image_tag = card.select_one("div.post-thumbnail img")
                image_url = None
                if image_tag and image_tag.has_attr("src"):
                    image_url = urljoin(source.news_source_url, image_tag["src"])

                # Ø§ÙØªØ­ ØµÙØ­Ø© Ø§Ù„Ù…Ù‚Ø§Ù„ Ù„Ø¬Ù„Ø¨ Ø§Ù„ØªÙØ§ØµÙŠÙ„ ÙˆØ§Ù„ØªØ§Ø±ÙŠØ®
                published_at = None
                article_content = title  # Ø§ÙØªØ±Ø§Ø¶ÙŠ

                if full_link != "#":
                    try:
                        driver.get(full_link)
                        time.sleep(2)
                        article_soup = BeautifulSoup(driver.page_source, "html.parser")

                        # Ø¬Ù„Ø¨ Ø§Ù„ÙÙ‚Ø±Ø§Øª ÙƒÙ…Ø­ØªÙˆÙ‰
                        paragraphs = article_soup.select("div.entry p")
                        if paragraphs:
                            article_content = "\n".join(
                                p.get_text(strip=True) for p in paragraphs
                            )

                        # Ø¬Ù„Ø¨ Ø§Ù„ØªØ§Ø±ÙŠØ®
                        from dateutil import parser

                        date_tag = article_soup.select_one("span.tie-date")
                        if date_tag:
                            date_text = date_tag.get_text(strip=True)
                            try:
                                published_at = parser.parse(date_text)
                            except Exception as err:
                                print(f"âš ï¸ Failed to parse date: {err} â€” '{date_text}'")
                    except Exception as e:
                        print(f"âš ï¸ Failed to fetch full article: {e}")

                articles.append(
                    {
                        "title": title,
                        "content": article_content,
                        "keywords": extract_keywords(title + " " + article_content),
                        "image": image_url,
                        "published_at": published_at,
                        "type": infer_article_type(title),
                        "category": None,
                        "url": full_link,
                    }
                )

            except Exception as e:
                print(f"âš ï¸ SANA card error: {e}")

    except Exception as e:
        print(f"âŒ Selenium failed for SANA: {e}")
    finally:
        driver.quit()

    return articles


# --------------Ø§Ù„Ø¬Ø²ÙŠØ±Ø©--------------
def scrape_aljazeera(source):

    url = source.news_source_url or "https://www.aljazeera.net/"
    print(f"ğŸ§­ Scraping Al-Jazeera from main page: {url}")

    headers = {"User-Agent": "Mozilla/5.0"}

    try:
        response = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.content, "html.parser")
    except Exception as e:
        print(f"âŒ Failed to load Al-Jazeera main page: {e}")
        return []

    cards = soup.select("a.u-clickable-card__link")
    print(f"ğŸ§© Found {len(cards)} cards")

    articles = []
    seen_links = set()

    for i, card in enumerate(cards[:15]):
        try:
            href = card.get("href")
            full_link = urljoin(url, href) if href else None

            if not full_link or full_link in seen_links:
                continue
            seen_links.add(full_link)

            title_tag = card.select_one(
                "h3.article-card__title span"
            ) or card.select_one("h3.article-card__title")
            title = title_tag.get_text(strip=True) if title_tag else "Ø¨Ø¯ÙˆÙ† Ø¹Ù†ÙˆØ§Ù†"
            if not title or title == "Ø¨Ø¯ÙˆÙ† Ø¹Ù†ÙˆØ§Ù†":
                continue

            # Ø¬Ù„Ø¨ ØµÙØ­Ø© Ø§Ù„Ù…Ù‚Ø§Ù„
            try:
                article_res = requests.get(full_link, headers=headers, timeout=10)
                article_soup = BeautifulSoup(article_res.content, "html.parser")

                # âœ… Ø¬Ù„Ø¨ Ø§Ù„Ù…Ø­ØªÙˆÙ‰
                paragraphs = article_soup.select("div[class*='wysiwyg'] p")
                content = (
                    "\n".join(p.get_text(strip=True) for p in paragraphs)
                    if paragraphs
                    else title
                )

                # âœ… Ø¬Ù„Ø¨ Ø§Ù„ØªØ§Ø±ÙŠØ®
                published_at = None
                date_tag = article_soup.select_one("span.article-dates__published")
                if date_tag:
                    try:
                        published_at = parser.parse(
                            date_tag.get_text(strip=True), dayfirst=True
                        )
                    except:
                        pass

                # âœ… Ø¬Ù„Ø¨ Ø§Ù„ØµÙˆØ±Ø© Ù…Ù† ØµÙØ­Ø© Ø§Ù„Ù…Ù‚Ø§Ù„
                image_url = None
                img_tag = article_soup.select_one(
                    "figure img"
                ) or article_soup.select_one("img")
                if img_tag and img_tag.has_attr("src"):
                    image_url = urljoin(full_link, img_tag["src"])

                articles.append(
                    {
                        "title": title,
                        "content": content,
                        "keywords": extract_keywords(title + " " + content),
                        "image": image_url,
                        "published_at": published_at,
                        "type": infer_article_type(title),
                        "category": None,
                        "url": full_link,
                    }
                )
                print(f"âœ… Article added: {title}")

            except Exception as e:
                print(f"âš ï¸ Failed to fetch full article: {e}")

        except Exception as e:
            print(f"âš ï¸ Error parsing card #{i+1}: {e}")

    print(f"\nğŸŸ¢ Total articles scraped: {len(articles)}")
    return articles


# --------------BBC--------------scrape_bbc_with_playwright
def scrape_bbc(soup=None, source=None):
    url = source.news_source_url
    print(f"ğŸ§­ Scraping BBC main page: {url}")

    try:
        response = requests.get(url, headers={"User-Agent": "Mozilla/5.0"})
        soup = BeautifulSoup(response.content, "html.parser")
    except Exception as e:
        print(f"âŒ Failed to load BBC main page: {e}")
        return []

    articles = []

    # âœ… Ø¹Ù†Ø§ØµØ± Ø§Ù„Ù…Ù‚Ø§Ù„Ø§Øª Ø¶Ù…Ù† Ø±ÙˆØ§Ø¨Ø· /arabic/articles/..
    cards = soup.select("a[href*='/arabic/articles/']")
    print(f"ğŸ§© Found {len(cards)} BBC article links")

    seen_links = set()
    fetch_limit = getattr(source, "fetch_limit", 3)

    for card in cards:
        try:
            full_link = urljoin(url, card["href"])
            if full_link in seen_links:
                continue  # Ù„ØªØ¬Ù†Ø¨ Ø§Ù„ØªÙƒØ±Ø§Ø±
            seen_links.add(full_link)

            title = card.get_text(strip=True)
            if not title or title == "Ø¨Ø¯ÙˆÙ† Ø¹Ù†ÙˆØ§Ù†":
                print("â›” Skipping article with no valid title")
                continue

            # Ø¬Ù„Ø¨ ØµÙØ­Ø© Ø§Ù„Ø®Ø¨Ø±
            try:
                article_res = requests.get(
                    full_link, headers={"User-Agent": "Mozilla/5.0"}
                )
                article_soup = BeautifulSoup(article_res.content, "html.parser")

                # âœ… Ø§Ù„Ù…Ø­ØªÙˆÙ‰ (article > p Ø£Ùˆ fallback Ø¥Ù„Ù‰ div > p)
                paragraphs = article_soup.select("article p") or article_soup.select(
                    "div p"
                )
                article_content = (
                    "\n".join(p.get_text(strip=True) for p in paragraphs)
                    if paragraphs
                    else title
                )

                if not paragraphs:
                    print(f"âš ï¸ No content found in: {full_link}")

                # Ø§Ù„ØªØ§Ø±ÙŠØ®
                published_at = None
                time_tag = article_soup.select_one("time")
                if time_tag and time_tag.has_attr("datetime"):
                    published_at = parser.parse(time_tag["datetime"])

                # Ø§Ù„ØµÙˆØ±Ø©
                image_url = None
                img_tag = article_soup.select_one("figure img")
                if img_tag and img_tag.has_attr("src"):
                    image_url = urljoin(full_link, img_tag["src"])

                # ØªØ®Ø²ÙŠÙ†
                articles.append(
                    {
                        "title": title,
                        "content": article_content,
                        "keywords": extract_keywords(title + " " + article_content),
                        "image": image_url,
                        "published_at": published_at,
                        "type": infer_article_type(title),
                        "category": None,
                        "url": full_link,
                    }
                )
                print(f"âœ… Article added: {title}")

                if len(articles) >= fetch_limit:
                    break  # Ù†ÙƒØªÙÙŠ Ø¨Ø¹Ø¯Ø¯ Ù…Ù‚Ø§Ù„Ø§Øª Ù…Ø­Ø¯Ø¯ Ø­Ø³Ø¨ Ø§Ù„Ù…ØµØ¯Ø±

            except Exception as e:
                print(f"âš ï¸ Failed to fetch article page: {e}")

        except Exception as e:
            print(f"âš ï¸ BBC card parsing error: {e}")

    print(f"\nğŸŸ¢ Total articles scraped: {len(articles)}")
    return articles


# --------------elsport--------------
def parse_arabic_datetime(date_str, time_str):
    month_map = {
        "ÙƒØ§Ù†ÙˆÙ† Ø§Ù„Ø«Ø§Ù†ÙŠ": "01",
        "Ø´Ø¨Ø§Ø·": "02",
        "Ø¢Ø°Ø§Ø±": "03",
        "Ù†ÙŠØ³Ø§Ù†": "04",
        "Ø£ÙŠØ§Ø±": "05",
        "Ø­Ø²ÙŠØ±Ø§Ù†": "06",
        "ØªÙ…ÙˆØ²": "07",
        "Ø¢Ø¨": "08",
        "Ø£ÙŠÙ„ÙˆÙ„": "09",
        "ØªØ´Ø±ÙŠÙ† Ø§Ù„Ø£ÙˆÙ„": "10",
        "ØªØ´Ø±ÙŠÙ† Ø§Ù„Ø«Ø§Ù†ÙŠ": "11",
        "ÙƒØ§Ù†ÙˆÙ† Ø§Ù„Ø£ÙˆÙ„": "12",
    }

    for ar_month, num in month_map.items():
        if ar_month in date_str:
            date_str = date_str.replace(ar_month, num)
            break

    # Ù…Ø«Ø§Ù„ Ø§Ù„Ù†Ø§ØªØ¬: "08 06 2025"
    parts = date_str.strip().split()
    if len(parts) >= 3:
        day = parts[-3]
        month = parts[-2]
        year = parts[-1]
        return parser.parse(f"{year}-{month}-{day} {time_str}")
    return None


def scrape_elsport(source, override_url=None):

    url = override_url or source.news_source_url
    headers = {"User-Agent": "Mozilla/5.0"}

    try:
        response = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.content, "html.parser")
    except Exception as e:
        print(f"âŒ Failed to load Elsport: {e}")
        return []

    articles = []
    cards = soup.select("li.newsfeed-main")
    print(f"ğŸ§© Found {len(cards)} cards on Elsport")

    for card in cards[:10]:
        try:
            link_tag = card.select_one("a.news-title")
            title = link_tag.get_text(strip=True) if link_tag else "Ø¨Ø¯ÙˆÙ† Ø¹Ù†ÙˆØ§Ù†"
            full_link = urljoin(url, link_tag["href"]) if link_tag else "#"

            image_tag = card.select_one("img")
            image_url = (
                urljoin(url, image_tag["src"])
                if image_tag and image_tag.has_attr("src")
                else None
            )

            article_content = title
            published_at = None

            if full_link != "#":
                try:
                    article_res = requests.get(full_link, headers=headers, timeout=10)
                    article_soup = BeautifulSoup(article_res.content, "html.parser")

                    # âœ… Ø¬Ù„Ø¨ Ø§Ù„ÙÙ‚Ø±Ø§Øª
                    # âœ… Ø¬Ù„Ø¨ Ø§Ù„ÙÙ‚Ø±Ø§Øª
                    # paragraphs = article_soup.select("div.article-text p")
                    paragraphs = article_soup.select("div.articleBody p")

                    if paragraphs:
                        article_content = "\n".join(
                            p.get_text(strip=True) for p in paragraphs
                        )

                    # âœ… Ø¬Ù„Ø¨ Ø§Ù„ØªØ§Ø±ÙŠØ® Ù…Ù† div.news-info span
                    # âœ… Ø¬Ù„Ø¨ Ø§Ù„ØªØ§Ø±ÙŠØ® ÙˆØ§Ù„ÙˆÙ‚Øª
                    date_tag = article_soup.select("strong.desktop-date")
                    time_tag = article_soup.select_one("strong.desktop-time")

                    if len(date_tag) >= 1 and time_tag:
                        date_text = date_tag[0].get_text(strip=True)
                        time_text = time_tag.get_text(strip=True)
                        published_at = parse_arabic_datetime(date_text, time_text)

                except Exception as e:
                    print(f"âš ï¸ Failed to fetch full article: {e}")

            if article_content.strip() == title.strip():
                print(f"âš ï¸ Warning: Content fallback to title for {title}")

            articles.append(
                {
                    "title": title,
                    "content": article_content,
                    "keywords": extract_keywords(title + " " + article_content),
                    "image": image_url,
                    "published_at": published_at,
                    "type": infer_article_type(title),
                    "category": None,
                    "url": full_link,
                }
            )

        except Exception as e:
            print(f"âš ï¸ Error parsing Elsport card: {e}")

    return articles


# --------------tishreen--syria-tv-----------
def parse_arabic_date(date_str):
    # Ø£Ø´Ù‡Ø± Ø¹Ø±Ø¨ÙŠØ© Ø´Ø§Ø¦Ø¹Ø©
    months = {
        "ÙƒØ§Ù†ÙˆÙ† Ø§Ù„Ø«Ø§Ù†ÙŠ": "01",
        "Ø´Ø¨Ø§Ø·": "02",
        "Ø¢Ø°Ø§Ø±": "03",
        "Ù†ÙŠØ³Ø§Ù†": "04",
        "Ø£ÙŠØ§Ø±": "05",
        "Ø­Ø²ÙŠØ±Ø§Ù†": "06",
        "ØªÙ…ÙˆØ²": "07",
        "Ø¢Ø¨": "08",
        "Ø£ÙŠÙ„ÙˆÙ„": "09",
        "ØªØ´Ø±ÙŠÙ† Ø§Ù„Ø£ÙˆÙ„": "10",
        "ØªØ´Ø±ÙŠÙ† Ø§Ù„Ø«Ø§Ù†ÙŠ": "11",
        "ÙƒØ§Ù†ÙˆÙ† Ø§Ù„Ø£ÙˆÙ„": "12",
    }

    import re
    from dateutil import parser

    try:
        match = re.match(r"(\d{1,2})-([^\d]+)-(\d{4})", date_str.strip())
        if match:
            day, ar_month, year = match.groups()
            month = months.get(ar_month.strip(), "01")
            date_fixed = f"{year}-{month}-{day.zfill(2)}"
            return parser.parse(date_fixed)
    except Exception as e:
        print(f"âŒ Failed to convert Arabic date: {e}")

    return None


def scrape_syria_tv_with_selenium(source, override_url=None):

    url = override_url or source.news_source_url
    print(f"ğŸ§­ Scraping Syria TV with Selenium from: {url}")

    options = Options()
    options.headless = True
    options.add_argument("--disable-blink-features=AutomationControlled")
    options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64)")
    service = Service("D:/chromedriver/chromedriver.exe")  # ğŸ” Ø¹Ø¯Ù‘Ù„ Ø¥Ø°Ø§ Ø¹Ù†Ø¯Ùƒ Ù…Ø³Ø§Ø± Ø¢Ø®Ø±
    driver = webdriver.Chrome(service=service, options=options)

    articles = []

    try:
        driver.get(url)
        time.sleep(5)
        soup = BeautifulSoup(driver.page_source, "html.parser")

        cards = soup.select("div.node--type-article")
        print(f"ğŸ§© Found {len(cards)} cards")

        for card in cards[:3]:
            try:
                title_tag = card.select_one("div.field--name-node-title h2 a")
                title = title_tag.get_text(strip=True) if title_tag else "Ø¨Ø¯ÙˆÙ† Ø¹Ù†ÙˆØ§Ù†"
                full_link = urljoin(url, title_tag["href"]) if title_tag else "#"

                image_tag = card.select_one("img.media__image")
                image_url = (
                    urljoin(url, image_tag["src"])
                    if image_tag and image_tag.has_attr("src")
                    else None
                )
                published_at = None
                date_tag = card.select_one("div.field--name-field-published-date")
                if date_tag:
                    raw_date = date_tag.get_text(strip=True)
                    print(f"ğŸ“… Raw date: {raw_date}")
                    try:
                        published_at = parse_arabic_date(raw_date)
                        print(f"âœ… Parsed date: {published_at}")
                    except Exception as e:
                        print(f"âš ï¸ Failed to parse date: {e}")

                article_content = title
                try:
                    driver.get(full_link)
                    time.sleep(3)
                    article_soup = BeautifulSoup(driver.page_source, "html.parser")

                    paragraphs = article_soup.select("div.field--name-body p")
                    article_content = "\n".join(
                        p.get_text(strip=True) for p in paragraphs
                    )
                except Exception as e:
                    print(f"âš ï¸ Failed to fetch full article with Selenium: {e}")

                articles.append(
                    {
                        "title": title,
                        "content": article_content,
                        "keywords": extract_keywords(title + " " + article_content),
                        "image": image_url,
                        "published_at": published_at,
                        "type": infer_article_type(title),
                        "category": None,
                        "url": full_link,
                    }
                )

                print(f"âœ… Article added: {title}")

            except Exception as e:
                print(f"âš ï¸ Card parsing error: {e}")

    except Exception as e:
        print(f"âŒ Selenium failed for Syria TV: {e}")

    finally:
        driver.quit()

    return articles
